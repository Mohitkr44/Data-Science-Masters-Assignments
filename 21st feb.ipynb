{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d361fb9-05bf-4176-b5e2-5de933527efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9eed4-cfeb-4710-b6a0-3defe0f493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "Web scraping is the process of extracting data from websites by using automated software or tools, which can access the website's HTML code, parse it, and extract the desired information. This technique can be used to collect data such as text, images, URLs, and other structured or unstructured data.\n",
    "Web scraping is used for a variety of reasons, such as:\n",
    "Market research: Companies can use web scraping to gather pricing information on products from competitor websites, or to collect customer reviews to improve their own products.\n",
    "\n",
    "Content aggregation: Web scraping can be used to gather news articles or blog posts from various sources and aggregate them on a single platform.\n",
    "\n",
    "Data analysis: Researchers or data scientists can use web scraping to collect data for analysis, such as social media data or information on public opinion.\n",
    "\n",
    "Here are three specific areas where web scraping is commonly used:\n",
    "E-commerce: Web scraping can be used to gather pricing information and product details from e-commerce websites to gain insights into the market and competitor pricing strategies.\n",
    "\n",
    "Social media monitoring: Web scraping can be used to monitor social media platforms for mentions of specific brands or products, which can help businesses to manage their online reputation or respond to customer inquiries.\n",
    "\n",
    "Academic research: Web scraping can be used to gather data for academic research, such as collecting data on public opinion or political sentiment from news websites or social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b3572-b88e-4631-9940-9220d565d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3ea2a-ddd6-4a9b-ae8b-1056d2373b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "MANUAL SCRAPING\n",
    "COPY-PASTING - In manual scraping, what you do is copy and paste web content. This is time-consuming and repetitive and begs for a more effective means of web scraping. It is however very effective because a website’s defences are targeted at automated scraping and not manual scraping techniques. Even with this benefit, manual scraping is hardly being done because it is time-consuming while automated scraping is quicker and cheaper.\n",
    "AUTOMATED SCRAPING\n",
    "HTML PARSING - There are many web scraping tools and libraries available that can extract data from websites. Some popular examples include BeautifulSoup, Scrapy, and Selenium in Python.\n",
    "\n",
    "DOM PARSING - DOM is short for Document Object Model and it defines the style structure and content of XML files. Scrapers make use of DOM parsers to get an in-depth view of a web page’s structure. They can also use a DOM parser to get nodes containing information and then use a tool like XPath to scrape web pages. Internet Explorer or Firefox browsers can be embedded to extract the entire web page or just parts of it.\n",
    "\n",
    "VERTICAL AGGREGATION - Vertical aggregation platforms are created by companies with access to large scale computing power to target specific verticals. Some companies run the platforms on the cloud. Bots creation and monitoring for specific verticals are done by these platforms without any human intervention. The quality of the bots is measured based on the quality of data they extract since they are created based on the knowledge base for the specific vertical.\n",
    "\n",
    "XPATH - XML Path Language is a query language that is used with XML documents. XPath can be used to navigate XML documents because of their tree-like structure by selecting nodes based on different parameters. XPath can be used together with DOM parsing to scrape an entire web page.\n",
    "\n",
    "GOOGLE SHEETS - Google sheets are a web scraping tool that is quite popular among web scrapers. From within sheets, a scraper can make use of IMPORT XML (,) function to scrape as much data as is needed from websites. This method is only useful when specific data or patterns are required from a website. You can also use this command to check if your website is secure from scraping.\n",
    "\n",
    "TEXT PATTERN MATCHING - This is a matching technique that involves the use of the UNIX grep command and is used with popular programming languages like Perl or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361f34b-9d69-499a-81fd-f888c69c2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3bdcb-6a33-4eb1-b3b5-f4f60dbdac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "Beautiful Soup is a Python library used for web scraping purposes. It is a popular parsing library that is used to extract data from HTML and XML documents. Beautiful Soup can parse the HTML code of a web page and extract the relevant data, such as links, images, and text, by providing a simple and intuitive interface.\n",
    "Beautiful Soup is used for web scraping because:\n",
    "It is easy to learn and use: Beautiful Soup is a beginner-friendly library that is easy to install and use. Its intuitive and flexible interface makes it easy to navigate the HTML code and extract the desired data.\n",
    "\n",
    "It can handle imperfect HTML code: Many web pages have imperfect HTML code, which can cause errors when parsing. Beautiful Soup can handle such imperfect HTML code and still extract the relevant data.\n",
    "\n",
    "It supports multiple parsing methods: Beautiful Soup supports multiple parsing methods, including HTML and XML parsing, which makes it versatile and useful for a wide range of web scraping tasks.\n",
    "\n",
    "It has a large community: Beautiful Soup has a large community of developers who contribute to its development and provide support. This makes it a reliable and stable library for web scraping.\n",
    "\n",
    "In summary, Beautiful Soup is a powerful and versatile web scraping library that can extract data from HTML and XML documents. Its ease of use, support for imperfect HTML code, and support for multiple parsing methods make it a popular choice among web scrapers.\n",
    "Example of Beautiful Soup to get top 5 result of search query Product in flipkart website here i have considered smart watch for men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4306fdc9-516b-4253-a0ac-56dc87b2b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd0904b-50e2-4033-bf6b-80b45c6681e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.flipkart.com/search?q=smart+watch+for+men'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL to scrape\n",
    "search_query = \"smart watch for men\"\n",
    "search_query = search_query.replace(\" \",\"+\")\n",
    "flpkt_url = \"https://www.flipkart.com/search?q=\"+search_query\n",
    "flpkt_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905d5354-66c1-4086-bbe2-5e7154cd1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlclient = urlopen(flpkt_url)\n",
    "flipkart_page = urlclient.read()\n",
    "flipkart_html = bs(flipkart_page,'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a5d4b7-8549-4b7e-be1d-94c84250e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting all product names\n",
    "product_titles = flipkart_html.find_all(\"div\",{\"class\":\"_4rR01T\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be612e76-6dc6-4055-8acc-a1958bc4bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "names = []\n",
    "for i in product_titles[0:5]:\n",
    "    print(f\"Result {counter}: {i.text}\")\n",
    "    names.append(i.text)\n",
    "    counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4815006e-8c6c-4d51-a58a-468bc1afb465",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting Top 5 urls\n",
    "product_urls = flipkart_html.find_all(\"a\",{\"class\":\"_1fQZEK\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e582937-00f5-4cd1-82df-6ced0599e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_urls = []\n",
    "for i in product_urls[0:5]:\n",
    "    u = \"https://www.flipkart.com\"+i['href']\n",
    "    top5_urls.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f558218b-76a5-43ec-890f-fc25158ae7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Title</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Product_Title, URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Product_Title':names,\"URL\":top5_urls})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4bc33-895b-4fc0-a6fb-50a2f31d27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829dfda-56a6-4bed-8bcc-ff56f2aa069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer:\n",
    "Flask is a web framework that is commonly used to build web applications and APIs in Python. While it is not required for web scraping, it can be useful in certain scenarios.\n",
    "In the context of a web scraping project, Flask is used to create a simple web interface that allows users to input URLs or search queries and view the scraped data in a user-friendly format. For example, you could create a Flask app that takes a search query for a product on Flipkart, scrapes the results page, and displays the relevant information (such as product names, prices, and ratings) in a table on the app's homepage.\n",
    "Using Flask in this way can make it easier to share the results of your web scraping project with others, as they can access the scraped data through a web interface rather than needing to run the scraping code themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c5b3c-4a40-4df7-8b84-137d9c458594",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ee32a-506f-4bdb-96fc-46d20cc9ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "Two AWS Services used in this project are:\n",
    "Elastic Beanstalk\n",
    "Code Pipeline\n",
    "1. Elastic Beanstalk\n",
    "Elastic Beanstalk is a fully managed service provided by Amazon Web Services (AWS) that allows developers to easily deploy, manage, and scale web applications and services written in popular programming languages like Java, Python, Node.js, PHP, Ruby, Go, and .NET. With Elastic Beanstalk, developers can focus on writing code without worrying about the underlying infrastructure, as the service handles provisioning and configuration of the resources needed to run the application.\n",
    "\n",
    "Here are some key features of Elastic Beanstalk:\n",
    "\n",
    "Platform as a Service (PaaS): Elastic Beanstalk abstracts away the underlying infrastructure and provides a simple interface for developers to deploy their applications. Developers simply upload their application code, and Elastic Beanstalk handles the rest, including provisioning the necessary resources (such as compute instances, load balancers, and databases) and configuring the environment.\n",
    "\n",
    "Multi-language Support: Elastic Beanstalk supports a wide range of programming languages, frameworks, and platforms, including Java, Python, Node.js, PHP, Ruby, Go, and .NET. It also supports popular web servers like Apache, Nginx, and IIS.\n",
    "\n",
    "Easy Deployment: Developers can deploy their applications to Elastic Beanstalk using a variety of methods, including the Elastic Beanstalk console, the AWS CLI, or APIs. Elastic Beanstalk supports versioning of deployments, so developers can roll back to a previous version if needed.\n",
    "\n",
    "Auto Scaling: Elastic Beanstalk automatically scales the application up or down based on demand, ensuring that the application is always available and responsive to users. It can also automatically balance traffic across multiple instances of the application to optimize performance.\n",
    "\n",
    "Monitoring and Logging: Elastic Beanstalk provides monitoring and logging capabilities that allow developers to monitor the health and performance of their application, and troubleshoot issues if they arise. It also integrates with other AWS services like CloudWatch and Elastic Load Balancing to provide a complete solution for monitoring and managing applications.\n",
    "\n",
    "Overall, Elastic Beanstalk is a powerful and flexible service that can help developers quickly and easily deploy and manage web applications and services on AWS.\n",
    "\n",
    "2. Code Pipeline\n",
    "AWS CodePipeline is a fully managed continuous delivery service provided by Amazon Web Services (AWS). It automates the release process for applications, enabling developers to rapidly and reliably build, test, and deploy their code changes.\n",
    "\n",
    "Here are some key features of AWS CodePipeline:\n",
    "\n",
    "Pipeline Creation: Developers can create custom pipelines for their applications, specifying the source code repository, build tools, testing frameworks, deployment targets, and other settings. They can also define the stages of the pipeline and the actions that should be performed in each stage.\n",
    "\n",
    "Source Code Integration: CodePipeline integrates with a wide range of source code repositories, including AWS CodeCommit, GitHub, and Bitbucket. Developers can configure their pipelines to automatically detect code changes in the repository and trigger the build and deployment process.\n",
    "\n",
    "Build and Test Automation: CodePipeline supports a variety of build and test tools, including AWS CodeBuild, Jenkins, and Bamboo. Developers can configure their pipelines to run automated tests as part of the build process, ensuring that code changes meet quality standards before being deployed.\n",
    "\n",
    "Deployment Automation: CodePipeline can deploy applications to a wide range of targets, including Amazon EC2 instances, AWS Elastic Beanstalk environments, and AWS Lambda functions. It can also integrate with other AWS services like AWS CodeDeploy and AWS CloudFormation to support more complex deployment scenarios.\n",
    "\n",
    "Continuous Monitoring: CodePipeline provides continuous monitoring of the pipeline and its stages, giving developers visibility into the progress of each stage and the status of each action. It also integrates with AWS CloudWatch to provide monitoring and alerting capabilities for the pipeline and the application.\n",
    "\n",
    "Overall, AWS CodePipeline is a powerful tool for automating the release process for applications, enabling developers to deploy changes quickly and reliably while maintaining high quality standards. By eliminating the need for manual intervention and automating many of the tedious and error-prone tasks involved in software deployment, CodePipeline can help teams deliver software faster and with fewer errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
